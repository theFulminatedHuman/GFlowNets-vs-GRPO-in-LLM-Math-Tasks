# GRPO vs GFlowNets: Mathematical Reasoning Benchmark

## Core Methodology

### GFlowNets Implementation
```python
class GFlowTrainer:
    def detailed_balance_loss(self, s, a, s_next):
        # Flow matching constraint
        log_F_s = self.flow_net(s)
        log_F_s_next = self.flow_net(s_next)
        log_policy = self.policy_net(s, a)
        
        # Detailed balance loss
        loss = (log_F_s + log_policy - log_F_s_next)**2
        return loss
```

**Expected Advantage:**  
Natural exploration of multiple solution paths through compositional policy

**Actual Limitation:**  
Struggled with exact mathematical operations due to:

- Credit assignment challenges in long trajectories
- Premature convergence to suboptimal flows

---

### GRPO Implementation
```python
def grpo_update(policy, optimizer, episodes, group_size=4):
    # Group rewards by problem type
    groups = split_into_groups(episodes, group_size)
    
    for group in groups:
        rewards = [e.reward for e in group]
        advantages = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-6)
        
        # Policy update
        for episode, adv in zip(group, advantages):
            loss = -episode.log_prob * adv
            loss.backward()
            
    optimizer.step()
```

**Key Strength:**  
Group-wise normalization handled varying problem difficulty effectively

---

## Empirical Results (Real Observations)

| Metric            | GFlowNets | GRPO |
|-------------------|-----------|------|
| Training Time (s) | 412       | 5253 |
| Memory Usage (GB) | 3.2       | 4.8  |
| GSM8K Accuracy    | 0.00*     | 0.12 |
| MATH Accuracy     | 0.00*     | 0.09 |

\*Failed to learn meaningful policies in our implementation

---

## Why GFlowNets Failed

**Mathematical Rigidity:**
```math
âˆ‡L = (F(s)Ï€(a|s) - F(s')Ï€^{-1}(s|s'))^2
```
- Couldn't handle precise numerical relationships

**Exploration Issues:**
- Preferred terminating early over complete solutions
- High variance in gradient estimates

---

## Why GRPO Worked Better

**Adaptive Normalization:**
```math
A_g = (R(s,a) - Î¼_g)/Ïƒ_g
```
Automatically scaled rewards per problem type

**Direct Policy Updates:**
```math
âˆ‡J(Î¸) = ð”¼[âˆ‡logÏ€(a|s)A_g]
```
More stable learning signal

---

## Key Lessons

- **For Mathematical Reasoning:**  
  Online updates (GRPO) > Flow matching (GFlowNets)  
  Exact operations need precise credit assignment

- **Implementation Challenges:**  
  GFlowNets require careful reward shaping  
  GRPO needs efficient group formation

---


```
